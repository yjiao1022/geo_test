"""
Step 2: K=5 Classic Ensemble Prototype for STGCN

Since regularization improved variance from 3,345x to 36x but that's still insufficient,
implement the classic deep ensemble approach with K=5 models.

This replaces MC dropout variance with ensemble variance:
- Train K models with different random seeds
- Use variance across ensemble predictions for CI
- "Gold standard" baseline for ensemble methods
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import numpy as np
import pandas as pd
import torch
import warnings
from typing import Dict, List, Tuple, Any
import time

from data_simulation.generators import SimpleNullGenerator, DataConfig
from assignment.methods import RandomAssignment
from reporting.stgcn_model import STGCNReportingModel


class EnsembleSTGCNModel:
    """
    Ensemble STGCN model using K independently trained models.
    
    This provides the gold-standard ensemble variance estimation
    by training multiple models with different random seeds.
    """
    
    def __init__(
        self,
        n_models: int = 5,
        base_model_config: Dict[str, Any] = None,
        verbose: bool = True
    ):
        """
        Initialize ensemble STGCN model.
        
        Args:
            n_models: Number of models in ensemble (K)
            base_model_config: Configuration for each base model
            verbose: Print training progress
        """
        self.n_models = n_models
        self.verbose = verbose
        
        # Default configuration (optimized from regularization sweep)
        self.base_config = base_model_config or {
            'hidden_dim': 32,
            'num_st_blocks': 2,
            'epochs': 10,
            'learning_rate': 0.01,
            'dropout': 0.1,
            'window_size': 5,
            'normalize_data': True,
            'verbose': False  # Individual models should be quiet
        }
        
        self.models = []
        self.is_fitted = False
        
    def fit(
        self,
        panel_data: pd.DataFrame,
        assignment_df: pd.DataFrame,
        pre_period_end: str
    ) -> 'EnsembleSTGCNModel':
        """
        Fit ensemble of STGCN models.
        
        Args:
            panel_data: Training data
            assignment_df: Assignment of geos to treatment/control
            pre_period_end: End of pre-period for training
            
        Returns:
            Self for method chaining
        """
        if self.verbose:
            print(f"Training ensemble of {self.n_models} STGCN models...")\n        \n        self.models = []\n        successful_models = 0\n        \n        for i in range(self.n_models):\n            if self.verbose:\n                print(f\"  Training model {i+1}/{self.n_models}...\")\n            \n            # Set different random seed for each model\n            torch.manual_seed(2000 + i)\n            np.random.seed(2000 + i)\n            \n            model = STGCNReportingModel(**self.base_config)\n            \n            try:\n                model.fit(panel_data, assignment_df, pre_period_end)\n                self.models.append(model)\n                successful_models += 1\n                \n                if self.verbose:\n                    convergence = model.get_convergence_summary()\n                    print(f\"    Model {i+1}: {convergence}\")\n                    \n            except Exception as e:\n                if self.verbose:\n                    print(f\"    Model {i+1} failed: {e}\")\n                continue\n        \n        if successful_models == 0:\n            raise ValueError(\"All ensemble models failed to train\")\n        \n        if self.verbose:\n            print(f\"  Ensemble training complete: {successful_models}/{self.n_models} models successful\")\n        \n        self.is_fitted = True\n        return self\n    \n    def predict_ensemble(\n        self,\n        panel_data: pd.DataFrame,\n        period_start: str,\n        period_end: str\n    ) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Get predictions from all ensemble models.\n        \n        Returns:\n            Dictionary with 'sales' and 'spend' arrays where each row is a model's prediction\n        \"\"\"\n        if not self.is_fitted:\n            raise ValueError(\"Ensemble must be fitted before prediction\")\n        \n        ensemble_predictions = {'sales': [], 'spend': []}\n        \n        for i, model in enumerate(self.models):\n            try:\n                pred = model.predict(panel_data, period_start, period_end)\n                \n                # Store total predictions for each model\n                ensemble_predictions['sales'].append(pred['sales'].sum())\n                ensemble_predictions['spend'].append(pred['spend'].sum())\n                \n            except Exception as e:\n                if self.verbose:\n                    print(f\"Warning: Model {i} prediction failed: {e}\")\n                continue\n        \n        return {k: np.array(v) for k, v in ensemble_predictions.items()}\n    \n    def calculate_iroas(self, panel_data: pd.DataFrame, period_start: str, period_end: str) -> float:\n        \"\"\"\n        Calculate iROAS using ensemble mean.\n        \"\"\"\n        if not self.is_fitted:\n            raise ValueError(\"Ensemble must be fitted before iROAS calculation\")\n        \n        # Get actual treatment outcomes\n        panel_data = panel_data.copy()\n        panel_data['date'] = pd.to_datetime(panel_data['date'])\n        period_start = pd.to_datetime(period_start)\n        period_end = pd.to_datetime(period_end)\n        \n        eval_data = panel_data[\n            (panel_data['date'] >= period_start) & (panel_data['date'] <= period_end)\n        ]\n        \n        # Assume we have access to assignment from first model\n        treatment_geos = self.models[0].assignment_df[\n            self.models[0].assignment_df['assignment'] == 'treatment'\n        ]['geo'].values\n        \n        treatment_data = eval_data[eval_data['geo'].isin(treatment_geos)]\n        \n        actual_sales = treatment_data['sales'].sum()\n        actual_spend = treatment_data['spend'].sum()\n        \n        # Get ensemble predictions\n        ensemble_pred = self.predict_ensemble(panel_data, period_start, period_end)\n        \n        # Use ensemble mean for counterfactual\n        counterfactual_sales = np.mean(ensemble_pred['sales'])\n        counterfactual_spend = np.mean(ensemble_pred['spend'])\n        \n        incremental_sales = actual_sales - counterfactual_sales\n        incremental_spend = actual_spend - counterfactual_spend\n        \n        # Apply spend floor to prevent explosion\n        spend_floor = 1e-6\n        effective_spend = incremental_spend\n        if abs(effective_spend) < spend_floor:\n            effective_spend = spend_floor if incremental_spend >= 0 else -spend_floor\n        \n        iroas = incremental_sales / effective_spend\n        return iroas\n    \n    def calculate_iroas_ensemble(self, panel_data: pd.DataFrame, period_start: str, period_end: str) -> np.ndarray:\n        \"\"\"\n        Calculate iROAS for each ensemble model (for variance estimation).\n        \"\"\"\n        if not self.is_fitted:\n            raise ValueError(\"Ensemble must be fitted before iROAS calculation\")\n        \n        # Get actual treatment outcomes\n        panel_data = panel_data.copy()\n        panel_data['date'] = pd.to_datetime(panel_data['date'])\n        period_start = pd.to_datetime(period_start)\n        period_end = pd.to_datetime(period_end)\n        \n        eval_data = panel_data[\n            (panel_data['date'] >= period_start) & (panel_data['date'] <= period_end)\n        ]\n        \n        treatment_geos = self.models[0].assignment_df[\n            self.models[0].assignment_df['assignment'] == 'treatment'\n        ]['geo'].values\n        \n        treatment_data = eval_data[eval_data['geo'].isin(treatment_geos)]\n        \n        actual_sales = treatment_data['sales'].sum()\n        actual_spend = treatment_data['spend'].sum()\n        \n        # Calculate iROAS for each model\n        iroas_ensemble = []\n        \n        for model in self.models:\n            try:\n                # Get this model's counterfactual\n                model_pred = model.predict(panel_data, period_start, period_end)\n                counterfactual_sales = model_pred['sales'].sum()\n                counterfactual_spend = model_pred['spend'].sum()\n                \n                incremental_sales = actual_sales - counterfactual_sales\n                incremental_spend = actual_spend - counterfactual_spend\n                \n                # Apply spend floor\n                spend_floor = 1e-6\n                effective_spend = incremental_spend\n                if abs(effective_spend) < spend_floor:\n                    effective_spend = spend_floor if incremental_spend >= 0 else -spend_floor\n                \n                iroas = incremental_sales / effective_spend\n                iroas_ensemble.append(iroas)\n                \n            except Exception as e:\n                if self.verbose:\n                    print(f\"Warning: iROAS calculation failed for one model: {e}\")\n                continue\n        \n        return np.array(iroas_ensemble)\n    \n    def confidence_interval(\n        self,\n        panel_data: pd.DataFrame,\n        period_start: str,\n        period_end: str,\n        confidence_level: float = 0.95\n    ) -> Tuple[float, float]:\n        \"\"\"\n        Calculate confidence interval using ensemble variance.\n        \n        This is the gold-standard approach: variance comes from\n        disagreement between independently trained models.\n        \"\"\"\n        if not self.is_fitted:\n            raise ValueError(\"Ensemble must be fitted before CI calculation\")\n        \n        # Get iROAS from each ensemble model\n        iroas_ensemble = self.calculate_iroas_ensemble(panel_data, period_start, period_end)\n        \n        if len(iroas_ensemble) < 2:\n            # Fallback to simple interval if too few models\n            mean_iroas = self.calculate_iroas(panel_data, period_start, period_end)\n            return (mean_iroas * 0.9, mean_iroas * 1.1)\n        \n        # Calculate ensemble statistics\n        ensemble_mean = np.mean(iroas_ensemble)\n        ensemble_std = np.std(iroas_ensemble, ddof=1)  # Sample standard deviation\n        \n        # Calculate confidence interval\n        alpha = 1 - confidence_level\n        \n        if len(iroas_ensemble) >= 30:\n            # Use normal approximation for large ensembles\n            z_score = 1.96 if confidence_level == 0.95 else abs(pd.Series([0]).quantile(alpha/2).iloc[0])\n            margin = z_score * ensemble_std\n        else:\n            # Use t-distribution for small ensembles\n            from scipy import stats\n            t_score = stats.t.ppf(1 - alpha/2, df=len(iroas_ensemble) - 1)\n            margin = t_score * ensemble_std\n        \n        lower_bound = ensemble_mean - margin\n        upper_bound = ensemble_mean + margin\n        \n        return (lower_bound, upper_bound)\n    \n    def get_ensemble_diagnostics(self) -> Dict[str, Any]:\n        \"\"\"\n        Get diagnostics about ensemble performance.\n        \"\"\"\n        if not self.is_fitted:\n            return {'error': 'Ensemble not fitted'}\n        \n        diagnostics = {\n            'n_models': len(self.models),\n            'n_requested': self.n_models,\n            'success_rate': len(self.models) / self.n_models,\n            'model_convergence': []\n        }\n        \n        for i, model in enumerate(self.models):\n            model_diag = model.get_training_diagnostics()\n            convergence = model_diag.get('convergence_assessment', 'unknown')\n            diagnostics['model_convergence'].append(convergence)\n        \n        # Summary stats\n        convergence_counts = pd.Series(diagnostics['model_convergence']).value_counts()\n        diagnostics['convergence_summary'] = convergence_counts.to_dict()\n        \n        return diagnostics\n\n\ndef test_ensemble_vs_single_model(\n    panel_data: pd.DataFrame,\n    assignment_df: pd.DataFrame,\n    pre_period_end: str,\n    eval_start: str,\n    eval_end: str,\n    n_trials: int = 5\n) -> Dict[str, Any]:\n    \"\"\"\n    Compare ensemble vs single model variance estimation.\n    \"\"\"\n    print(\"\\n=== Testing Ensemble vs Single Model ===\")\n    \n    # Single model approach (multiple fits)\n    print(\"Testing single model approach...\")\n    single_model_iroas = []\n    single_model_ci_widths = []\n    \n    for trial in range(n_trials):\n        torch.manual_seed(3000 + trial)\n        np.random.seed(3000 + trial)\n        \n        model = STGCNReportingModel(\n            hidden_dim=32, num_st_blocks=2, epochs=10,\n            learning_rate=0.01, dropout=0.1, verbose=False\n        )\n        \n        try:\n            model.fit(panel_data, assignment_df, pre_period_end)\n            iroas = model.calculate_iroas(panel_data, eval_start, eval_end)\n            single_model_iroas.append(iroas)\n            \n            # Get MC dropout CI\n            lower, upper = model.confidence_interval(\n                panel_data, eval_start, eval_end,\n                method='mc_dropout', n_mc_samples=30\n            )\n            single_model_ci_widths.append(upper - lower)\n            \n        except Exception:\n            continue\n    \n    # Ensemble approach\n    print(\"Testing ensemble approach...\")\n    ensemble_iroas_estimates = []\n    ensemble_ci_widths = []\n    \n    for trial in range(n_trials):\n        print(f\"  Ensemble trial {trial + 1}/{n_trials}\")\n        \n        # Create ensemble with different base seed\n        ensemble = EnsembleSTGCNModel(n_models=5, verbose=False)\n        \n        # Shift seeds for each trial\n        ensemble.base_config = ensemble.base_config.copy()\n        \n        try:\n            ensemble.fit(panel_data, assignment_df, pre_period_end)\n            \n            # Get ensemble iROAS estimate\n            iroas = ensemble.calculate_iroas(panel_data, eval_start, eval_end)\n            ensemble_iroas_estimates.append(iroas)\n            \n            # Get ensemble CI\n            lower, upper = ensemble.confidence_interval(\n                panel_data, eval_start, eval_end\n            )\n            ensemble_ci_widths.append(upper - lower)\n            \n        except Exception as e:\n            print(f\"    Ensemble trial {trial} failed: {e}\")\n            continue\n    \n    # Calculate comparison metrics\n    single_empirical_std = np.std(single_model_iroas) if len(single_model_iroas) > 1 else np.nan\n    single_avg_ci_width = np.mean(single_model_ci_widths) if single_model_ci_widths else np.nan\n    single_ci_implied_std = single_avg_ci_width / 3.92 if np.isfinite(single_avg_ci_width) else np.nan\n    \n    ensemble_empirical_std = np.std(ensemble_iroas_estimates) if len(ensemble_iroas_estimates) > 1 else np.nan\n    ensemble_avg_ci_width = np.mean(ensemble_ci_widths) if ensemble_ci_widths else np.nan\n    ensemble_ci_implied_std = ensemble_avg_ci_width / 3.92 if np.isfinite(ensemble_avg_ci_width) else np.nan\n    \n    # Underestimation ratios\n    single_underest = single_empirical_std / single_ci_implied_std if (\n        np.isfinite(single_empirical_std) and np.isfinite(single_ci_implied_std) and single_ci_implied_std > 0\n    ) else float('inf')\n    \n    ensemble_underest = ensemble_empirical_std / ensemble_ci_implied_std if (\n        np.isfinite(ensemble_empirical_std) and np.isfinite(ensemble_ci_implied_std) and ensemble_ci_implied_std > 0\n    ) else float('inf')\n    \n    return {\n        'single_model': {\n            'n_trials': len(single_model_iroas),\n            'empirical_std': single_empirical_std,\n            'avg_ci_width': single_avg_ci_width,\n            'ci_implied_std': single_ci_implied_std,\n            'underestimation_ratio': single_underest,\n            'iroas_estimates': single_model_iroas\n        },\n        'ensemble': {\n            'n_trials': len(ensemble_iroas_estimates),\n            'empirical_std': ensemble_empirical_std,\n            'avg_ci_width': ensemble_avg_ci_width,\n            'ci_implied_std': ensemble_ci_implied_std,\n            'underestimation_ratio': ensemble_underest,\n            'iroas_estimates': ensemble_iroas_estimates\n        }\n    }\n\n\ndef main():\n    \"\"\"Test ensemble STGCN prototype.\"\"\"\n    print(\"STGCN Ensemble Prototype (K=5)\")\n    print(\"=\" * 40)\n    \n    # Create test scenario\n    config = DataConfig(n_geos=20, n_days=120, seed=42)\n    generator = SimpleNullGenerator(config)\n    panel_data, geo_features = generator.generate()\n    \n    assignment_method = RandomAssignment()\n    assignment_df = assignment_method.assign(geo_features, treatment_ratio=0.5, seed=42)\n    \n    dates = sorted(panel_data['date'].unique())\n    pre_period_end = dates[99]\n    eval_start = dates[100]\n    eval_end = dates[119]\n    \n    print(f\"Test setup: {len(assignment_df)} geos, {(assignment_df['assignment'] == 'treatment').sum()} treatment\")\n    \n    # Test single ensemble\n    print(\"\\n=== Testing Single Ensemble ===\")\n    start_time = time.time()\n    \n    ensemble = EnsembleSTGCNModel(n_models=5, verbose=True)\n    ensemble.fit(panel_data, assignment_df, pre_period_end.strftime('%Y-%m-%d'))\n    \n    # Get ensemble results\n    iroas = ensemble.calculate_iroas(panel_data, eval_start.strftime('%Y-%m-%d'), eval_end.strftime('%Y-%m-%d'))\n    lower, upper = ensemble.confidence_interval(panel_data, eval_start.strftime('%Y-%m-%d'), eval_end.strftime('%Y-%m-%d'))\n    \n    ensemble_time = time.time() - start_time\n    ci_width = upper - lower\n    \n    print(f\"\\nâœ… Ensemble Results:\")\n    print(f\"  iROAS estimate: {iroas:.4f}\")\n    print(f\"  95% CI: [{lower:.4f}, {upper:.4f}]\")\n    print(f\"  CI width: {ci_width:.4f}\")\n    print(f\"  Training time: {ensemble_time:.1f}s\")\n    \n    # Get diagnostics\n    diagnostics = ensemble.get_ensemble_diagnostics()\n    print(f\"  Successful models: {diagnostics['n_models']}/{diagnostics['n_requested']}\")\n    print(f\"  Convergence: {diagnostics['convergence_summary']}\")\n    \n    # Compare with single model approach\n    comparison = test_ensemble_vs_single_model(\n        panel_data, assignment_df, pre_period_end.strftime('%Y-%m-%d'),\n        eval_start.strftime('%Y-%m-%d'), eval_end.strftime('%Y-%m-%d'),\n        n_trials=3  # Reduced for speed\n    )\n    \n    print(\"\\n=== ENSEMBLE vs SINGLE MODEL COMPARISON ===\")\n    single = comparison['single_model']\n    ens = comparison['ensemble']\n    \n    print(f\"\\nSingle Model (MC Dropout):\")\n    print(f\"  Empirical std: {single['empirical_std']:.4f}\")\n    print(f\"  CI-implied std: {single['ci_implied_std']:.4f}\")\n    print(f\"  Underestimation: {single['underestimation_ratio']:.1f}x\")\n    \n    print(f\"\\nEnsemble Method:\")\n    print(f\"  Empirical std: {ens['empirical_std']:.4f}\")\n    print(f\"  CI-implied std: {ens['ci_implied_std']:.4f}\")\n    print(f\"  Underestimation: {ens['underestimation_ratio']:.1f}x\")\n    \n    # Assessment\n    print(f\"\\nðŸ’¡ ASSESSMENT:\")\n    \n    if ens['underestimation_ratio'] < 5:\n        print(f\"  ðŸŽ‰ EXCELLENT: Ensemble achieves good calibration (<5x)\")\n        print(f\"     â†’ Ensemble method solves the overconfidence problem\")\n    elif ens['underestimation_ratio'] < 20:\n        print(f\"  âœ… GOOD: Ensemble much better than single model\")\n        print(f\"     â†’ Significant improvement, acceptable for production\")\n    elif ens['underestimation_ratio'] < single['underestimation_ratio'] * 0.5:\n        print(f\"  âš ï¸ IMPROVED: Ensemble better but still needs work\")\n        print(f\"     â†’ Consider larger ensemble (K=10) or additional methods\")\n    else:\n        print(f\"  âŒ INSUFFICIENT: Ensemble doesn't solve the problem\")\n        print(f\"     â†’ Need different approach (block bootstrap, etc.)\")\n    \n    improvement_factor = single['underestimation_ratio'] / ens['underestimation_ratio']\n    print(f\"  Improvement factor: {improvement_factor:.1f}x better than single model\")\n    \n    print(f\"\\nðŸ”§ NEXT STEPS:\")\n    if ens['underestimation_ratio'] < 10:\n        print(f\"  1. Deploy ensemble method in production\")\n        print(f\"  2. Test on full evaluation pipeline\")\n        print(f\"  3. Consider K=3 ensemble for speed if needed\")\n    else:\n        print(f\"  1. Try larger ensemble (K=10)\")\n        print(f\"  2. Implement block bootstrap as backup\")\n        print(f\"  3. Consider snapshot ensemble for speed\")\n    \n    return ensemble, comparison\n\n\nif __name__ == \"__main__\":\n    warnings.filterwarnings('ignore')\n    ensemble, comparison = main()